\subsection{Datenerfassung aus dem Web}\label{subsec:webdaten}
Der Prozess der Webdatenerfassung umfasst die Datenextraktion aus unstrukturierten bwz. semi-strukturierten Webdokumenten (z.B. eine Webseite oder eine E-Mail) und die Transformation der erfassten Daten in eine strukturierte Form zur späteren Verwendung \cite[S.301]{ferrara2014}. Im Folgenden werden die allgemeinen Probleme bei der Erfassung von Webdaten angesprochen. Anschließend werden generelle Paradigmen der Datenerfassung erläutert, nämlich das Baumparadigma, Web-Wrapper und hybride Systeme.\\
Bei der Erfassung der Webdaten gibt es mehrere Faktoren, die berücksichtigt werden sollen. Ferrara et al. \cite[S.302]{ferrara2014} identifizieren folgende Herausforderungen:
\begin{itemize}
\item \textit{Automatisierungsgrad}: Die Erfassung der Webdaten soll regelmäßig von einem menschlichen Experten überwacht werden, um die Genauigkeit der Daten zu gewährleisten.
\item \textit{Skalierbarkeit}: Bei umfangreichen Webressourcen soll innerhalb kurzer Zeit eine große Datenmenge bearbeitet werden.
\item \textit{Datenschutz}: Wenn es um die Erfassung personenbezogener Daten geht (z.B. bei sozialen Netzwerken wie Facebook), soll die Privatsphäre des Individuums nicht beeinträchtigt werden.
\item \textit{Änderung der Ressourcenstruktur}: Die Struktur von Webressourcen ändert sich oft. Die Datenerfassungsmethoden für das Web sollen eine gewisse Flexibilität besitzen, um weiterhin korrekt zu funktionieren.
\item \textit{Trainingsdaten}: Beim Einsatz des maschinellen Lernens ist eine ausreichende Trainingsmenge an Webseiten erforderlich, die manuell vorbereitet wird. Dies ist eine aufwendige und fehleranfällige Aufgabe.
\end{itemize}
Das Baumparadigma nutzt die Baumstruktur einer Webseite, um die gewünschten Daten zu erfassen. Dabei handelt es sich um Dokumente, die in der \acf{HTML} beschrieben sind. Eine HTML-Seite wird als \acf{DOM}\footnote{https://www.w3.org/DOM} definiert. Die Idee des DOM besteht darin, dass eine HTML-Webseite in einer Baumstruktur dargestellt wird, die mittels HTML-Tags (z.B. Button-Tag) ausgezeichnet ist. Tags können weitere Tags beinhalten und bilden somit eine hierarchische Struktur, die eine effiziente Datensuche auf einer HTML-Seite ermöglicht \cite[S.303]{ferrara2014}.\\
Da HTML Eigenschaften einer \acf{XML} integriert, kann die \acf{XPath}\footnote{https://www.w3.org/TR/xpath} zur Navigation im DOM eingesetzt werden. In einem XPath-Ausdruck können beliebige Elemente einer HTML-Webseite ausgewählt werden. In Abbildung \ref{xpath} werden zwei Beispiele dargestellt. Im ersten Fall (A) wird genau ein Element einer Tabelle (die erste Zelle in der ersten Zeile) ausgewählt. Im Beispiel (B) werden mehrere Elemente (alle Zellen der zweiten Zeile) angesprochen \cite[S.303]{ferrara2014}.
\begin{figure}[H] 
	\centering
	\includegraphics[width=1.0\textwidth]{images/xpath.png}
	\caption{XPath im Dokumentenbaum, \cite[S.304]{ferrara2014}}
	\label{xpath}
\end{figure} 
Der größte Nachteil von XPath besteht darin, dass XPath-Ausdrücke strikt an die DOM-Struktur gebunden sind. Wenn eine Änderung im DOM stattfindet, funktioniert der von der Änderung betroffene Ausdruck nicht mehr. Aus diesem Grund müssen die XPath-Ausdrücke nach jeder Veränderung der HTML-Webseite manuell angepasst werden. Hinsichtlich dieses Problems wurden im letzten Release von XPath\footnote{https://www.w3.org/TR/xpath20} relative XPath-Ausdrücke eingeführt \cite[S.304]{ferrara2014}.\\
Ein weiterer Ansatz, die Webdaten zu erfassen, stellt ein Web-Wrapper dar. Dieser umfasst in der Regel einen oder mehrere Algorithmen, die zur Datenerfassung aus den Webdokumenten eingesetzt werden. Anschließend werden die erfassten Daten in eine strukturierte Form transformiert und für die weitere Nutzung gespeichert. Ein Web-Wrapper umfasst folgende Schritte \cite[S.305]{ferrara2014}:
\begin{itemize}
\item[1.]\textit{Generierung}: Definition des Wrappers.
\item[2.]\textit{Ausführung}: Datenerfassung mithilfe des Wrappers.
\item[3.]\textit{Wartung}: Anpassung des Wrappers bei der Änderung der DOM-Struktur.
\end{itemize}
Nach Ferrara et al. \cite[S.306]{ferrara2014} kann ein Web-Wrapper mittels folgender Ansätze generiert und ausgeführt werden:
\begin{itemize}
\item \textit{Reguläre Ausdrücke}: Daten werden gemäß Regeln (expressions) gewonnen. Im Rahmen dieser Arbeit wird sich auf diesen Ansatz der regulären Ausdrücke beschränkt.
\item \textit{Logikbasierter Ansatz}: Zur Datenerfassung wird eine Wrapper-Programmiersprache eingesetzt (wrapper programming language).
\item \textit{Baumbasierter Ansatz}: Es wird die Annahme getroffen, dass bestimmte Bereiche im DOM generell für Daten zuständig sind. Die Identifikation und Datenextraktion aus diesen Bereichen ist der Gegenstand des baumbasierten Ansatzes.  
\item \textit{Maschinelles Lernen}: Daten werden mithilfe eines Lernalgorithmus und einer Trainingsmenge erfasst.
\end{itemize} 
Reguläre Ausdrücke ermöglichen die Erkennung von Mustern in unstrukturierten bzw. semi-strukturierten Dokumenten unter Verwendung von Regeln, die z.B. in Form von Wortgrenzen oder HTML-Tags definiert werden. Der Vorteil der regulären Ausdrücke besteht in der Möglichkeit, ein beliebiges Element auf einer Webseite anzusprechen. Außerdem bieten einige Implementierungen ein grafisches Benutzerinterface, sodass der Benutzer die Elemente auf einfache Weise auswählen kann. Die für die Erkennung benötigten Regeln werden dann automatisch generiert. Eine mögliche Umsetzung des Wrappers wird in \cite{sahuguet1999} mit W4F vorgestellt. Dieses Tool verfügt über eine Hilfsmethode, die den Benutzer bei der Auswahl der Elemente unterstützt. Auf Basis der ausgewählten Elemente werden die Regeln erstellt. Allerdings sind die Regeln in Bezug auf DOM-Änderungen nicht flexibel und können sehr schnell verletzt werden \cite[S.306]{ferrara2014}.\\
In der dritten Phase geht es um die Anpassung des Web-Wrappers im Hinblick auf Veränderungen der DOM-Struktur. Die Anpassung erfolgt entweder manuell oder zum Teil automatisiert. Ferrara et al. \cite[S.308]{ferrara2014} betonen, dass der Automatisierungsgrad der Wartung besonders kritisch ist. Bei einer kleinen Dokumentenanzahl ist die manuelle Anpassung noch akzeptabel. Allerdings ist die manuelle Wartung bei einer großen Menge von Dokumenten nicht mehr realisierbar.\\
Ein Beispiel der automatisierten Wrapper-Wartung wird in \cite{meng2003} mit dem System namens SG-WRAM (Schema-Guided Wrapper Maintenance for Web-Data Extraction) vorgestellt. Die Architektur von SG-WRAM wird in Abbildung \ref{wram} dargestellt.
\begin{figure}[H] 
	\centering
	\includegraphics[width=1.0\textwidth]{images/wram.png}
	\caption{Die Architektur von SG-WRAM, \cite[S.2]{meng2003}}
	\label{wram}
\end{figure} 
Im ersten Schritt werden die HTML-Seite, das XML-Schema und das Mapping zwischen diesen in den Wrapper-Generator eingegeben. Das XML-Schema wird durch eine \acf{DTD} beschrieben. Daraufhin generiert das System die Regeln (Wrapper) für die gegebene Webseite, um die Daten zu erfassen und in einer XML-Datei gemäß der DTD zu speichern. Neben der Datenextraktion werden zusätzlich die Probleme bei der Extraktion erfasst, um ein Wiederherstellungsprotokoll für fehlgeschlagene Regelerzeugungen zu generieren. Wenn das Protokoll die Fehler beseitigt, wird die Datenerfassung fortgesetzt. Bei auftretenden Fehlern werden Warnungen und Benachrichtigungen angezeigt, dass die Regeln nicht mehr funktionieren. In diesem Fall müssen die Regeln manuell von einem Experten angepasst werden \cite[S.308]{ferrara2014}.\\
Als Ausblick wird ein hybrides System der Webdatenerfassung angesprochen. Ein Beispiel des hybriden Ansatzes ist RoadRunner von \cite{crescenzi2001}. Dies kann sowohl mit der Trainingsmenge eines Nutzers als auch mit selbst erstellten Trainingsdaten ausgeführt werden. Das Verfahren arbeitet gleichzeitig mit zwei HTML-Seiten und analysiert die Gemeinsam\-keit\-en und die Unterschiede zwischen diesen, um die Muster zu finden. Im Allgemeinen kann das Verfahren die Daten aus beliebigen Quellen erfassen, solange mindestens zwei Seiten mit ähnlicher Struktur gegeben sind. Da Webseiten normalerweise dynamisch auf Basis eines Templates generiert werden, befinden sich relevante Daten in gleichen oder ähnlichen Bereichen \cite[S.309]{ferrara2014}.