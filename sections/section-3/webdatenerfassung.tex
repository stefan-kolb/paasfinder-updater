\subsection{Datenerfassung aus dem Web}\label{subsec:webdaten}
Der Prozess der Webdatenerfassung umfasst die Datenextraktion aus unstrukturierten bwz. semi-strukturierten Webdokumenten (z.B. eine Webseite oder eine E-Mail) und die Transformation der erfassten Daten in eine strukturierte Form für spätere Verwendung \cite[S.301]{ferrara2014}. Im Folgenden werden die allgemeinen Probleme bei der Erfassung der Webdaten angesprochen. Anschließend werden generelle Paradigmen der Datenerfassung erläutert, nämlich Baumparadigma, Web-Wrapper und hybrides System.\\
Bei der Erfassung der Webdaten gibt es mehrere Faktoren, die berücksichtigt werden sollen. Ferrara et al. \cite[S.302]{ferrara2014} identifizieren folgende Herausforderungen:
\begin{itemize}
\item \textit{Automatisierungsgrad}: Die Erfassung der Webdaten soll regelmäßig von einem menschlichen Experten überwacht werden, um die Genauigkeit der Daten zu gewährleisten.
\item \textit{Skalierbarkeit}: Bei umfangreichen Webressourcen soll innerhalb kurzer Zeit eine große Datenmenge bearbeitet werden.
\item \textit{Datenschutz}: Wenn es um die Erfassung der personenbezogenen Daten geht (bei sozialen Netzwerken wie Facebook), soll die Privatsphäre des Individuums nicht beeinträchtigt werden.
\item \textit{Änderung der Ressourcenstruktur}: Die Struktur von Webressourcen ändert sich oft. Die Datenerfassungsmethoden für das Web sollen eine gewisse Flexibilität besitzen, um weiterhin korrekt zu funktionieren.
\item \textit{Trainingsdaten}: Bei Einsatz des maschinellen Lernens ist eine ausreichende Trainingsmenge an Webseiten erforderlich, die manuell vorbereitet wird. Dies ist eine aufwendige und fehleranfällige Aufgabe.
\end{itemize}
Das Baumparadigma nutzt die Baumstruktur einer Webseite aus, um die gewünschten Daten zu erfassen. Dabei handelt es sich um Dokumente, die in \acf{HTML} beschrieben sind. Eine HTML-Seite wird als \acf{DOM}\footnote{https://www.w3.org/DOM} definiert. Die Idee des DOM besteht darin, dass eine HTML-Webseite einen Baum darstellt, der mittels HTML-Tags (z.B. Button-Tag) ausgezeichnet wird. Tags können weitere Tags beinhalten und bilden somit eine hierarchische Struktur. Diese hierarchische Baumstruktur ermöglicht eine effiziente Datensuche in einer HTML-Seite \cite[S.303]{ferrara2014}.\\
Da HTML Eigenschaften einer \acf{XML} integriert, kann \acf{XPath}\footnote{https://www.w3.org/TR/xpath} für die Navigation im DOM eingesetzt werden. In einem XPath-Ausdruck können beliebige Elemente einer HTML-Webseite ausgewählt werden. In Abbildung \ref{xpath} werden zwei Beispiele dargestellt. Im ersten Fall (A) wird genau ein Element (die erste Zelle in der ersten Zeile) ausgewählt. Im Beispiel (B) werden mehrere Elemente (alle Zellen der zweiten Zeile) angesprochen \cite[S.303]{ferrara2014}.
\begin{figure}[H] 
	\centering
	\includegraphics[width=1.0\textwidth]{images/xpath.png}
	\caption{XPath im Dokumentenbaum, \cite[S.304]{ferrara2014}}
	\label{xpath}
\end{figure} 
Der Hauptnachteil von XPath besteht darin, dass XPath-Ausdrücke strikt an die DOM-Struktur gebunden sind. Wenn eine Änderung im DOM stattfindet, funktioniert der von der Änderung betroffene Ausdruck nicht mehr. Aus diesem Grund müssen die XPath-Ausdrücke nach jeder Veränderung der HTML-Webseite manuell angepasst werden. Hinsichtlich dieses Problems wurden im letzten Release von XPath\footnote{https://www.w3.org/TR/xpath20} relative XPath-Ausdrücke eingeführt \cite[S.304]{ferrara2014}.\\
Ein weiterer Ansatz, die Webdaten zu erfassen, stellt ein Web-Wrapper dar. Ein Web-Wrapper umfasst in der Regel einen oder mehrere Algorithmen, die zur Datenerfassung aus den Webdokumenten eingesetzt werden. Anschließend werden die erfassten Daten in eine strukturierte Form transformiert und für die weitere Nutzung gespeichert. Ein Web-Wrapper umfasst folgende Schritte \cite[S.305]{ferrara2014}:
\begin{itemize}
\item[1.]\textit{Generierung}: Definition des Wrappers.
\item[2.]\textit{Ausführung}: Datenerfassung mithilfe des Wrappers.
\item[3.]\textit{Wartung}: Anpassung des Wrappers bei der Änderung der DOM-Struktur.
\end{itemize}
Nach Ferrara et al. \cite[S.306]{ferrara2014} kann ein Web-Wrapper mittels folgender Ansätze generiert und ausgeführt werden:
\begin{itemize}
\item \textit{Reguläre Ausdrücke}: Daten werden gemäß Regeln (expressions) gewonnen. Im Rahmen dieser Arbeit wird sich auf diesen Ansatz der regulären Ausdrücke beschränkt.
\item \textit{Logikbasierter Ansatz}: Zur Datenerfassung wird eine Wrapper-Programmiersprache eingesetzt (wrapper programming language).
\item \textit{Baumbasierter Ansatz}: Es wird die Annahme getroffen, dass bestimmte Bereiche im DOM generell für Daten zuständig sind. Die Identifikation und Datenextraktion aus diesen Bereichen ist der Gegenstand des baumbasierten Ansatzes.  
\item \textit{Maschinelles Lernen}: Daten werden mithilfe eines Lernalgorithmus und einer Trainingsmenge erfasst.
\end{itemize} 
Reguläre Ausdrücke ermöglichen die Erkennung von Mustern in unstrukturierten bzw. semi-strukturierten Dokumenten unter Verwendung von Regeln, die z.B. in Form von Wortgrenzen oder HTML-Tags definiert werden. Der Vorteil der regulären Ausdrücke besteht in der Möglichkeit, ein beliebiges Element auf einer Webseite anzusprechen. Außerdem bieten einige Implementierungen ein grafisches Benutzerinterface, sodass der Benutzer die Elemente auf einfache Weise auswählen kann. Die für die Erkennung benötigten Regeln werden dann automatisch generiert. Eine mögliche Umsetzung des Wrappers wird in \cite{sahuguet1999} mit W4F vorgestellt. Dieses Tool verfügt über eine Hilfsmethode, die den Benutzer bei der Auswahl der Elemente unterstützt. Auf Basis der ausgewählten Elemente werden die Regeln erstellt. Allerdings sind die Regeln in Bezug auf DOM-Änderungen nicht flexibel und können sehr schnell verletzt werden \cite[S.306]{ferrara2014}.\\
In der dritten Phase geht es um die Anpassung des Web-Wrappers im Hinblick auf Veränderungen der DOM-Struktur. Die Anpassung erfolgt entweder manuell oder in gewisser Weise automatisiert. Ferrara et al. \cite[S.308]{ferrara2014} betonen, dass der Automatisierungsgrad der Wartung besonders kritisch ist. Bei einer kleinen Dokumentenanzahl ist die manuelle Anpassung noch akzeptabel. Allerdings ist die manuelle Wartung bei einer großen Menge von Dokumenten nicht mehr realisierbar.\\
Ein Beispiel der automatisierten Wrapper-Wartung wird in \cite{meng2003} mit dem System namens SG-WRAM (Schema-Guided Wrapper Maintenance for Web-Data Extraction) vorgestellt. Die Architektur vom SG-WRAM wird in Abbildung \ref{wram} dargestellt.
\begin{figure}[H] 
	\centering
	\includegraphics[width=0.95\textwidth]{images/wram.png}
	\caption{Die Architektur vom SG-WRAM, \cite[S.2]{meng2003}}
	\label{wram}
\end{figure} 
Im ersten Schritt werden die HTML-Seite, das XML-Schemata und das Mapping zwischen diesen in den Wrapper-Generator eingegeben. Das XML-Schema wird durch \acf{DTD} beschrieben. Daraufhin generiert das System die Regeln (Wrapper) für die gegebene Webseite, um die Daten zu erfassen und in einer XML-Datei gemäß der DTD-Datei zu speichern. Neben der Datenextraktion werden zusätzlich die Probleme bei der Extraktion erfasst, um ein Wiederherstellungsprotokoll für die fehlgeschlagenen Regeln zu erzeugen. Wenn das Protokoll die Fehler beseitigt wird die Datenerfassung fortgesetzt. Bei auftretenden Fehlern werden Warnungen und Benachrichtigungen angezeigt, dass die Regeln nicht mehr funktionieren. In diesem Fall müssen die Regeln manuell von einem Experten angepasst werden \cite[S.308]{ferrara2014}.\\
Als Ausblick wird ein hybrides System der Webdatenerfassung angesprochen. Ein Beispiel des hybriden Ansatzes stellt RoadRunner von \cite{crescenzi2001} dar. RoadRunner kann sowohl mit der Trainingsmenge von einem Nutzer als auch mit selbst erstellten Trainingsdaten ausgeführt werden. Das Verfahren arbeitet gleichzeitig mit zwei HTML-Seiten und analysiert die Gemeinsamkeiten und die Unterschiede zwischen diesen Seiten, um die Muster zu finden. Im Allgemeinen kann das Verfahren die Daten aus beliebigen Quellen erfassen, solange mindestens zwei Seiten mit ähnlicher Struktur gegeben sind. Da Webseiten normalerweise dynamisch auf Basis eines Templates generiert werden, befinden sich relevante Daten in gleichen oder ähnlichen Bereichen \cite[S.309]{ferrara2014}.