\section{Fazit}\label{sec:Fazit}
In der vorliegenden Arbeit wurden die Methoden und Verfahren betrachten, die im Kontext von wissensbasierten sowie Expertensystemen zur Automatisierung der Datenerfassung denkbar sind. Grundsätzlich wird die Arbeit in zwei Bereichen unterteilt, nämlich den theoretischen und den praktischen Teil. \\
Im Rahmen des theoretischen Teils wurden die Grundlagen von wissensbasierten und im Speziellen Expertensystemen umfassend analysiert. In Bezug auf das Thema dieser Arbeit wurde im Weiteren auf zwei Komponenten des Expertensystems konzentriert, und zwar die Wissensbasis und die Wissenserwerbskomponente. Im Verlauf der Arbeit hat sich herausgestellt, dass das maschinelle Lernen für die Wissensbasiserweiterung eine bedeutende Rolle spielt. In \cite{tecuci1992} und \cite{castro2001} wurde allerdings betont, dass das maschinelle Lernen in Kooperation mit einem fachlichen Experten am besten funktioniert. Ein anderer Ansatz wird in \cite{gebus2009} vorgestellt. In dieser Studie liegt der Fokus auf der Mensch-Computer-Interaktion. Um Erfahrungswissen aus der Produktionsabteilung zu erfassen, verwenden die Autoren eine graphische Benutzerschnittstelle. Eine andere Forschungsrichtung, die immer spannender wird, ist die Automatisierung der Datenerfassung aus dem Web. Diese Tatsache ist in der ersten Linie durch die enorm schnelle Entwicklung des Web begründet. Ferrara et al \cite{ferrara2014} haben einen umfassenden Überblick in diesem Bereich geschaffen. generell gibt es drei Ansätze, die bei der Erfassung der Webdaten eingesetzt werden, nämlich Baumparadigma, Web Wrapper und hybride Verfahren, die den Einsatz von Web-Wrapper und des maschinellen Lernens kombinieren.\\
Im praktischen Teil gibt es um die Anwendung der erworbenen Erkenntnisse auf ein wissensbasiertes System \textit{PaaSfinder}. Als erstes wurde die Idee der Wissensträgerschnittstelle umgesetzt, sodass der Nutzer der Wissensbasis von \textit{PaaSfinder} ohne Informatikvorkenntnisse beitragen kann. Die Implementierung umfasst zwei Teile. Im ersten Teil wurde das Frontend entwickelt, das die Daten sich dynamisch aktualisieren lassen und für das eventuelle Absenden zwischenspeichert werden. Im zweiten Teil wurde ein Service als REST-API implementiert, der für die Weiterleitung der Daten an das Git-Repository zuständig ist. Der Service verwendet wiederum die API von Github, um die Daten abzuschicken. Im weiteren wurden die Möglichkeiten analysiert, die eine Relevanz zur Automatisierung der Datenerfassung bei \textit{PaaSfinder} aufweisen. Es wurden drei Informationsquellen betrachtet, nämlich die Webseiten von PaaS-Anbietern, Web-Feeds und soziale Netzwerke. Die Webseiten bieten einen Informationsmehrwert, wenn es sich um einen nicht erfassten PaaS-Anbietern handelt. Für die Aktualisierungen auf der Webseite sind eher zeitlich geordnete Web-Feeds geeignet. Eine Alternative zu den Web-Feeds stellen soziale Netzwerke dar. Beispielsweise bietet Twitter eine hervorragende REST-API, die man zur Datenextraktion verwenden kann. 